{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "class DataLoaderHelper:\n",
        "    def __init__(self, train_data_dir,test_data_dir, input_size, batch_size, augmentation):\n",
        "        self.data_dir = train_data_dir\n",
        "        self.test_dir = test_data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.augmentation = augmentation\n",
        "        self.input_size = input_size  # tuple like (224, 224)\n",
        "\n",
        "        self.transform = self.get_transform()\n",
        "        self.train_data, self.val_data = self.load_train_val_data()\n",
        "        self.test_data = self.load_test_data()\n",
        "\n",
        "    def get_transform(self):\n",
        "        if self.augmentation:\n",
        "            transforms_list = [\n",
        "                transforms.RandomResizedCrop(self.input_size),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(30),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        else:\n",
        "            transforms_list = [\n",
        "                transforms.Resize(self.input_size),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "\n",
        "        transforms_list.append(\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        )\n",
        "        return transforms.Compose(transforms_list)\n",
        "\n",
        "    def load_train_val_data(self):\n",
        "        full_dataset = datasets.ImageFolder(root=self.data_dir, transform=self.transform)\n",
        "        total_size = len(full_dataset)\n",
        "        indices = list(range(total_size))\n",
        "\n",
        "        train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "        # print(f\"Total: {total_size} | Train: {len(train_idx)} | Val: {len(val_idx)}\")\n",
        "        return Subset(full_dataset, train_idx), Subset(full_dataset, val_idx)\n",
        "\n",
        "    def load_test_data(self):\n",
        "        return datasets.ImageFolder(root=self.test_dir, transform=self.transform)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        train_loader = DataLoader(self.train_data, batch_size=self.batch_size,\n",
        "                                  shuffle=True, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(self.val_data, batch_size=self.batch_size,\n",
        "                                shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "        test_loader = DataLoader(self.test_data, batch_size=self.batch_size,\n",
        "                                     shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "        return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "hwiZQDU1uN-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FlexibleCNN(nn.Module):\n",
        "    def __init__(self, num_filters, filter_sizes,dropout,activation, batch_norm, input_size,fc_hidden_sizes,num_classes):\n",
        "        #Initialize the pytorch neural network..\n",
        "        super(FlexibleCNN, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.activation = activation\n",
        "        self.batch_norm = batch_norm\n",
        "        self.num_filters = num_filters # list\n",
        "        self.filter_sizes = filter_sizes #list\n",
        "        self.input_size=input_size #tuple\n",
        "        self.fc_hidden_sizes=fc_hidden_sizes #list\n",
        "        self.num_classes=num_classes\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        #Call the functions to create Convolution layers and Fully Connected layers\n",
        "        # self.conv_layers = self.create_conv_layers()\n",
        "        # self.fc_layers = self.create_fc_layers()\n",
        "        self.conv_layers = nn.Sequential(*self.create_conv_layers())\n",
        "        self.fc_layers = nn.Sequential(*self.create_fc_layers())\n",
        "\n",
        "\n",
        "    def create_conv_layers(self):\n",
        "        layers = []\n",
        "        channels = 3  #RGB\n",
        "\n",
        "        for idx, (filters, size) in enumerate(zip(self.num_filters, self.filter_sizes)):\n",
        "\n",
        "            #For each layer we are adding the below there.\n",
        "\n",
        "            #Number of filters and filter size.\n",
        "            layers.append(nn.Conv2d(channels, filters, size,stride=1, padding=0))\n",
        "\n",
        "            if self.activation == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif self.activation == 'elu':\n",
        "                layers.append(nn.ELU())\n",
        "            elif self.activation == 'selu':\n",
        "                layers.append(nn.SELU())\n",
        "            elif self.activation == 'silu':\n",
        "                layers.append(nn.SiLU())\n",
        "            elif self.activation == 'gelu':\n",
        "                layers.append(nn.GELU())\n",
        "            else:\n",
        "                layers.append(nn.Mish())\n",
        "\n",
        "            # You can add new activation functions If you want\n",
        "\n",
        "            layers.append(nn.MaxPool2d(2))\n",
        "\n",
        "            if self.batch_norm:\n",
        "                layers.append(nn.BatchNorm2d(filters))\n",
        "\n",
        "            channels = filters\n",
        "\n",
        "\n",
        "        # return nn.Sequential(*layers)\n",
        "        return layers\n",
        "\n",
        "    def create_fc_layers(self):\n",
        "        # Calculate flattened size from conv layers\n",
        "        flattened_size = self.output_conv_layers()\n",
        "\n",
        "        layers = []\n",
        "        features = flattened_size\n",
        "\n",
        "        for hidden_size in self.fc_hidden_sizes:\n",
        "            # Add linear layer\n",
        "            layers.append(nn.Linear(features, hidden_size))\n",
        "\n",
        "            # Add activation function\n",
        "            if self.activation == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif self.activation == 'elu':\n",
        "                layers.append(nn.ELU())\n",
        "            elif self.activation == 'selu':\n",
        "                layers.append(nn.SELU())\n",
        "            elif self.activation == 'silu':\n",
        "                layers.append(nn.SiLU())\n",
        "            elif self.activation == 'gelu':\n",
        "                layers.append(nn.GELU())\n",
        "            else:\n",
        "                layers.append(nn.Mish())\n",
        "\n",
        "            # Add dropout\n",
        "            layers.append(nn.Dropout(self.dropout))\n",
        "\n",
        "            features = hidden_size\n",
        "\n",
        "        # Final output layer (no activation, no batch norm)\n",
        "        layers.append(nn.Linear(features, self.num_classes))\n",
        "\n",
        "        # return nn.Sequential(*layers)\n",
        "        return layers\n",
        "\n",
        "\n",
        "\n",
        "    def output_conv_layers(self):\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 3, *self.input_size)\n",
        "            conv_output = self.conv_layers(dummy_input)\n",
        "            return conv_output.view(1, -1).size(1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x= self.flatten(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "LssB7GjluUXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, val_loader, optimizer_name, learning_rate, num_epochs,weight_decay):\n",
        "        # Set device\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model=torch.nn.DataParallel(model,device_ids = [0,1]).to(device)\n",
        "        # self.model = model.to(self.device)\n",
        "\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.num_epochs = num_epochs\n",
        "        self.weight_decay= weight_decay\n",
        "        self.learning_rate=learning_rate\n",
        "        self.train_loss_history = []\n",
        "        self.val_loss_history = []\n",
        "        self.train_acc_history = []\n",
        "        self.val_acc_history = []\n",
        "\n",
        "        # Initialize optimizer\n",
        "        if optimizer_name.lower() == 'adam':\n",
        "            self.optimizer = optim.Adam(model.parameters(), lr=self.learning_rate,weight_decay=self.weight_decay)\n",
        "        elif optimizer_name.lower() == 'nadam':\n",
        "            self.optimizer = optim.NAdam(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "        elif optimizer_name.lower() == 'rmsprop':\n",
        "            self.optimizer = optim.RMSprop(model.parameters(), lr=self.learning_rate,weight_decay=self.weight_decay)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train_epoch(self):\n",
        "        #Initialize the training requirements that we have defined in model\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in tqdm(self.train_loader, desc=\"Training\"):\n",
        "            images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(images)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            #weight update\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(self.train_loader)\n",
        "        epoch_acc = correct / total\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(self.val_loader, desc=\"Validating\"):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(self.val_loader)\n",
        "        epoch_acc = correct / total\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
        "\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "            val_loss, val_acc = self.validate()\n",
        "\n",
        "            # Store history for analysis\n",
        "            self.train_loss_history.append(train_loss)\n",
        "            self.val_loss_history.append(val_loss)\n",
        "            self.train_acc_history.append(train_acc)\n",
        "            self.val_acc_history.append(val_acc)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.4f}\")\n",
        "            torch.cuda.empty_cache()\n",
        "\n"
      ],
      "metadata": {
        "id": "fADYz2YRud2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66AC-qpMunix",
        "outputId": "5684ee0b-1052-4ce4-82b9-7d5d44bab95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# directory='/content/drive/MyDrive/Ass2_dataset/nature_12K/inaturalist_12K/train'\n",
        "# director2='/content/drive/MyDrive/Ass2_dataset/nature_12K/inaturalist_12K/val'\n",
        "# input_dim=(224,224)\n",
        "# num_classes=10\n",
        "\n",
        "# data_loader = DataLoaderHelper(\n",
        "#             directory,test_data_dir=director2,\n",
        "#             input_size=input_dim,\n",
        "#             batch_size=64,\n",
        "#             augmentation=True\n",
        "#         )"
      ],
      "metadata": {
        "id": "uvxDwx17_-bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bY6N9L4EApG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import torch\n",
        "import os\n",
        "# from model import FlexibleCNN\n",
        "# from data_loader import DataLoaderHelper\n",
        "# from model_train import Trainer\n",
        "\n",
        "input_dim=(400,400)\n",
        "num_classes=10\n",
        "train_directory='/content/drive/MyDrive/Ass2_dataset/nature_12K/inaturalist_12K/train'\n",
        "test_directory='/content/drive/MyDrive/Ass2_dataset/nature_12K/inaturalist_12K/val'\n",
        "# Sweep configuration dictionary for wandb\n",
        "sweep_configuration = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'cnn-hyperparameter-tuning',\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'num_filters': {\n",
        "          'values': [[64,128,256,512, 1024], [32,32,32,32,32],[32,64,64,128,128],[128,128,64,64,32],[32,64,128,256,512]]\n",
        "        },\n",
        "        'filter_sizes': {\n",
        "          'values': [[3,3,3,3,3], [5,5,5,5,5], [5,3,5,3,5]]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0, 0.0005, 0.5]\n",
        "        },\n",
        "        'augmentation': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0, 0.2, 0.4]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu', 'elu', 'selu', 'silu', 'gelu','mish']\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['nadam', 'adam', 'rmsprop']\n",
        "        },\n",
        "        'batch_norm':{\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32, 64]\n",
        "        },\n",
        "        'fc_hidden_sizes':{\n",
        "            'values': [128, 256, 512]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def train_sweep(config=None):\n",
        "    with wandb.init(config=config) as run:\n",
        "        config = wandb.config\n",
        "\n",
        "\n",
        "        run.name = \"optimizer {} activation {} num_filters {} dropout {} filter_sizes {} batch_size {} augmentation {} weight_decay {} batch_norm {} \".format(\n",
        "            config.optimizer,\n",
        "            config.activation,\n",
        "            config.num_filters,\n",
        "            config.dropout,\n",
        "            config.filter_sizes,\n",
        "            config.batch_size,\n",
        "            config.augmentation,\n",
        "            config.weight_decay,\n",
        "            config.batch_norm\n",
        "          )\n",
        "        # Initialize data loaders\n",
        "        data_loader = DataLoaderHelper(\n",
        "            train_directory,test_data_dir=test_directory,\n",
        "            input_size=input_dim,\n",
        "            batch_size=config.batch_size,\n",
        "            augmentation=config.augmentation\n",
        "        )\n",
        "        train_loader, val_loader,test_loader = data_loader.get_dataloaders()\n",
        "\n",
        "        hidden_sizes = [config.fc_hidden_sizes]\n",
        "        # Initialize model\n",
        "        model = FlexibleCNN(\n",
        "            num_filters=config.num_filters,\n",
        "            filter_sizes=config.filter_sizes,\n",
        "            dropout=config.dropout,\n",
        "            activation=config.activation,\n",
        "            batch_norm=config.batch_norm,\n",
        "            input_size=input_dim,\n",
        "            fc_hidden_sizes=hidden_sizes,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            optimizer_name=config.optimizer,\n",
        "            learning_rate=config.learning_rate,\n",
        "            num_epochs=10,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        trainer.train()\n",
        "\n",
        "        # Log final metrics\n",
        "        for epoch in range(10):\n",
        "            wandb.log({\n",
        "                'train_accuracy': trainer.train_acc_history[epoch]*100,\n",
        "                'train_loss': trainer.train_loss_history[epoch],\n",
        "                'val_accuracy': trainer.val_acc_history[epoch]*100,\n",
        "                'val_loss': trainer.val_loss_history[epoch],\n",
        "                'epoch' : epoch\n",
        "            })\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    sweep_id = wandb.sweep(sweep_configuration, project=\"DA6401-Assignment-2\")\n",
        "\n",
        "\n",
        "    # Start sweep\n",
        "    wandb.agent('mes9cvi4', function=train_sweep, count=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bCvCU-eEujzc",
        "outputId": "a4d92bd2-dc9d-4164-aa10-2e7fae5243bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ym7ukbbt\n",
            "Sweep URL: https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/sweeps/ym7ukbbt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: odf2o3xd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_hidden_sizes: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [3, 5, 3, 5, 3]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: [32, 64, 128, 256, 512]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250413_144805-odf2o3xd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/runs/odf2o3xd' target=\"_blank\">soft-sweep-4</a></strong> to <a href='https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/sweeps/mes9cvi4' target=\"_blank\">https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/sweeps/mes9cvi4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2' target=\"_blank\">https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/sweeps/mes9cvi4' target=\"_blank\">https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/sweeps/mes9cvi4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/runs/odf2o3xd' target=\"_blank\">https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/runs/odf2o3xd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:34<00:00,  1.62it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:33<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.4845 | Train Acc: 23.0154\n",
            "Val Loss: 2.2228 | Val Acc: 28.2000\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:29<00:00,  1.67it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:36<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.0317 | Train Acc: 33.0041\n",
            "Val Loss: 2.1635 | Val Acc: 30.4000\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:31<00:00,  1.65it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:34<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.4792 | Train Acc: 49.0561\n",
            "Val Loss: 2.0676 | Val Acc: 33.1500\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:39<00:00,  1.57it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:37<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8979 | Train Acc: 71.3589\n",
            "Val Loss: 2.1772 | Val Acc: 33.1000\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:37<00:00,  1.59it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:34<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5118 | Train Acc: 85.4232\n",
            "Val Loss: 2.2239 | Val Acc: 35.3500\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:30<00:00,  1.66it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:33<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2676 | Train Acc: 93.8367\n",
            "Val Loss: 2.2628 | Val Acc: 34.7000\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:30<00:00,  1.66it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:34<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1655 | Train Acc: 96.8371\n",
            "Val Loss: 2.3669 | Val Acc: 35.0500\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:30<00:00,  1.66it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:33<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0922 | Train Acc: 98.6748\n",
            "Val Loss: 2.3983 | Val Acc: 34.8500\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:31<00:00,  1.65it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:33<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0596 | Train Acc: 99.3749\n",
            "Val Loss: 2.4196 | Val Acc: 35.8000\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [02:32<00:00,  1.64it/s]\n",
            "Validating: 100%|██████████| 63/63 [00:34<00:00,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0447 | Train Acc: 99.5124\n",
            "Val Loss: 2.4866 | Val Acc: 34.5500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▂▃▅▇▇████</td></tr><tr><td>train_loss</td><td>█▇▅▃▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▆▆█▇▇▇█▇</td></tr><tr><td>val_loss</td><td>▄▃▁▃▄▄▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>99.51244</td></tr><tr><td>train_loss</td><td>0.04469</td></tr><tr><td>val_accuracy</td><td>34.55</td></tr><tr><td>val_loss</td><td>2.48664</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">soft-sweep-4</strong> at: <a href='https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/runs/odf2o3xd' target=\"_blank\">https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/runs/odf2o3xd</a><br> View project at: <a href='https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2' target=\"_blank\">https://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250413_144805-odf2o3xd/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "TzbN3pDT6pDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}