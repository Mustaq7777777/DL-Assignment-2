{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing required Libraries"
      ],
      "metadata": {
        "id": "F61NMYIr4IEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch.amp import GradScaler, autocast"
      ],
      "metadata": {
        "id": "vOyINfQf4HqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Configuration"
      ],
      "metadata": {
        "id": "0I1AXFAd4Y6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive for data access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define path to dataset\n",
        "BASE_PATH = '/content/drive/MyDrive/DL-Assignment2-data/inaturalist_12K'"
      ],
      "metadata": {
        "id": "ahtqNs0h4hGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility Functions"
      ],
      "metadata": {
        "id": "v3SHKQ6t4rrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate output dimensions after convolution operation\n",
        "def calculate_output_dimensions(input_size, kernel_size, stride=1, padding=0):\n",
        "    \"\"\"Calculate the output dimensions after applying convolution\"\"\"\n",
        "    return math.floor((input_size - kernel_size + 2*padding) / stride) + 1"
      ],
      "metadata": {
        "id": "bRlYbsEC4uA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation(train, validation and test)"
      ],
      "metadata": {
        "id": "EvAhFmPo43D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(cfg):\n",
        "    \"\"\"\n",
        "    Prepare data loaders for training, validation and testing\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration object containing data parameters\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_loader, val_loader, test_loader)\n",
        "    \"\"\"\n",
        "    # Define transformations based on augmentation flag\n",
        "    if cfg.augmentation:\n",
        "        # More aggressive transformations for training\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(30),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        # Basic transformations without augmentation\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # Validation transforms (no augmentation needed)\n",
        "    val_transforms = transforms.Compose([\n",
        "        transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = datasets.ImageFolder(os.path.join(BASE_PATH, 'train'), transform=train_transforms)\n",
        "    test_dataset = datasets.ImageFolder(os.path.join(BASE_PATH, 'val'), transform=val_transforms)\n",
        "\n",
        "    # Split training data to create validation set\n",
        "    indices = list(range(len(train_dataset)))\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.2,  # 20% for validation\n",
        "        stratify=train_dataset.targets,  # Maintain class distribution\n",
        "        random_state=42  # For reproducibility\n",
        "    )\n",
        "\n",
        "    # Create subsets\n",
        "    train_subset = Subset(train_dataset, train_indices)\n",
        "    val_subset = Subset(train_dataset, val_indices)\n",
        "\n",
        "    # Get number of CPU cores for worker calculation\n",
        "    num_workers = min(2, os.cpu_count() or 1)  # Use at most 2 workers to avoid warning\n",
        "\n",
        "    # Create and return data loaders\n",
        "    return (\n",
        "        DataLoader(train_subset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                   num_workers=num_workers, pin_memory=True),\n",
        "        DataLoader(val_subset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                   num_workers=num_workers, pin_memory=True),\n",
        "        DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                   num_workers=num_workers, pin_memory=True)\n",
        "    )"
      ],
      "metadata": {
        "id": "seD17lJT4-Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution Nueral Network implemented as class"
      ],
      "metadata": {
        "id": "oVTkiTMh5Oyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Neural Network with configurable architecture\n",
        "    - Variable number of convolutional layers\n",
        "    - Configurable filter sizes and counts\n",
        "    - Choice of activation functions\n",
        "    - Optional batch normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Initialize lists to hold network components\n",
        "        self.conv_blocks = nn.ModuleList()\n",
        "\n",
        "        # Track dimensions for proper sizing of fully connected layer\n",
        "        in_channels = 3  # RGB input\n",
        "        current_size = cfg.img_size\n",
        "\n",
        "        # Create convolutional blocks\n",
        "        for i, (out_channels, kernel_size) in enumerate(zip(cfg.num_filters, cfg.filter_sizes)):\n",
        "            # Create a block with conv, optional batchnorm, activation, and pooling\n",
        "            block = self._create_conv_block(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                use_batchnorm=cfg.batch_norm,\n",
        "                activation=cfg.activation\n",
        "            )\n",
        "            self.conv_blocks.append(block)\n",
        "\n",
        "            # Update dimensions for next layer\n",
        "            current_size = calculate_output_dimensions(current_size, kernel_size, padding=1)\n",
        "            current_size = calculate_output_dimensions(current_size, 2, stride=2)  # pooling\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Adaptive pooling ensures fixed size regardless of input dimensions\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "        # Fully connected classification layers\n",
        "        self.classifier = self._create_classifier(\n",
        "            in_channels * 6 * 6,  # Flattened feature maps\n",
        "            cfg.fc_hidden_sizes,\n",
        "            10,  # Number of classes\n",
        "            cfg.dropout,\n",
        "            cfg.batch_norm,\n",
        "            cfg.activation\n",
        "        )\n",
        "\n",
        "    def _create_conv_block(self, in_channels, out_channels, kernel_size,\n",
        "                           use_batchnorm=True, activation='relu'):\n",
        "        \"\"\"Create a convolutional block with optional batch normalization\"\"\"\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
        "        ]\n",
        "\n",
        "        if use_batchnorm:\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        layers.append(self._get_activation_function(activation))\n",
        "        layers.append(nn.MaxPool2d(2, 2))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _create_classifier(self, in_features, hidden_size, num_classes,\n",
        "                           dropout_rate, use_batchnorm, activation):\n",
        "        \"\"\"Create the classifier part of the network\"\"\"\n",
        "        layers = [\n",
        "            nn.Linear(in_features, hidden_size),\n",
        "        ]\n",
        "\n",
        "        if use_batchnorm:\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "\n",
        "        layers.extend([\n",
        "            self._get_activation_function(activation),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        ])\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _get_activation_function(self, name):\n",
        "        \"\"\"Return the appropriate activation function based on name\"\"\"\n",
        "        activation_functions = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'gelu': nn.GELU(),\n",
        "            'silu': nn.SiLU(),\n",
        "            'mish': nn.Mish(),\n",
        "            'elu': nn.ELU(),\n",
        "            'selu': nn.SELU()\n",
        "        }\n",
        "        return activation_functions.get(name.lower(), nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        # Pass input through convolutional blocks\n",
        "        for block in self.conv_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Global pooling and flatten\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Classification\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "cOHE-rhM5Vrp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}