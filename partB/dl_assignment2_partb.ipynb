{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpguj+L64U1aA3RblRkAeJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mustaq7777777/DL-Assignment-2/blob/main/partB/dl_assignment2_partb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Statements"
      ],
      "metadata": {
        "id": "cbH1_0radwS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "ynqWJrGRd0VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SET UP and Configuration"
      ],
      "metadata": {
        "id": "zowGUh_Ud86_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive for data access\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "wandb.login(key=\"c4db2008beb715972687303f6cbced62af338b92\")\n",
        "\n",
        "# Define path to dataset\n",
        "BASE_PATH = '/kaggle/input/nature-12k1/inaturalist_12K'\n",
        "\n",
        "# Default configuration dictionary with hyperparameters\n",
        "DEFAULT_CONFIG = {\n",
        "    \"batch_size\": 64,                         # Number of samples per batch\n",
        "    \"learning_rate\": 1e-4,                    # Learning rate for optimizer\n",
        "    \"augmentation\": True,                     # Whether to use data augmentation\n",
        "}\n"
      ],
      "metadata": {
        "id": "6kJfjcYpeEjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility"
      ],
      "metadata": {
        "id": "__-m0UfXeQb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate output dimensions after convolution operation\n",
        "def calculate_output_dimensions(input_size, kernel_size, stride=1, padding=0):\n",
        "    \"\"\"Calculate the output dimensions after applying convolution\"\"\"\n",
        "    return math.floor((input_size - kernel_size + 2*padding) / stride) + 1"
      ],
      "metadata": {
        "id": "-i1J4vVUeUWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation"
      ],
      "metadata": {
        "id": "db2DnOwseYbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(cfg):\n",
        "    \"\"\"\n",
        "    Prepare data loaders for training, validation and testing\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration object containing data parameters\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_loader, val_loader, test_loader)\n",
        "    \"\"\"\n",
        "    # Define transformations based on augmentation flag\n",
        "    if cfg.augmentation:\n",
        "        # More aggressive transformations for training\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(30),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        # Basic transformations without augmentation\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # Validation transforms (no augmentation needed)\n",
        "    val_transforms = transforms.Compose([\n",
        "        transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = datasets.ImageFolder(os.path.join(BASE_PATH, 'train'), transform=train_transforms)\n",
        "    test_dataset = datasets.ImageFolder(os.path.join(BASE_PATH, 'val'), transform=val_transforms)\n",
        "\n",
        "    # Split training data to create validation set\n",
        "    indices = list(range(len(train_dataset)))\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.2,  # 20% for validation\n",
        "        stratify=train_dataset.targets,  # Maintain class distribution\n",
        "        random_state=42  # For reproducibility\n",
        "    )\n",
        "\n",
        "    # Create subsets\n",
        "    train_subset = Subset(train_dataset, train_indices)\n",
        "    val_subset = Subset(train_dataset, val_indices)\n",
        "\n",
        "    # Get number of CPU cores for worker calculation\n",
        "    num_workers = min(2, os.cpu_count() or 1)  # Use at most 2 workers to avoid warning\n",
        "\n",
        "    # Create and return data loaders\n",
        "    return (\n",
        "        DataLoader(train_subset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                   num_workers=num_workers, pin_memory=True),\n",
        "        DataLoader(val_subset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                   num_workers=num_workers, pin_memory=True),\n",
        "        DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                   num_workers=num_workers, pin_memory=True)\n",
        "    )"
      ],
      "metadata": {
        "id": "on88k9Vsed__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Trained model Resnet50"
      ],
      "metadata": {
        "id": "YLu4guIcel2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(strategy):\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Common classifier replacement\n",
        "    num_ftrs = model.fc.in_features\n",
        "\n",
        "    # Strategy 1: Freeze all except final layer\n",
        "    if strategy == 1:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        model.fc = nn.Linear(num_ftrs, 10)\n",
        "        return model, model.fc.parameters()"
      ],
      "metadata": {
        "id": "_jfrPDmUep_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Function"
      ],
      "metadata": {
        "id": "5gYul--Se4U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    \"\"\"Main training function that handles the entire training process\"\"\"\n",
        "    # Initialize wandb with default configuration\n",
        "    wandb.init(config=DEFAULT_CONFIG, reinit=True)\n",
        "    cfg = wandb.config\n",
        "\n",
        "    # Set fixed image size\n",
        "    cfg.img_size = 224\n",
        "\n",
        "    # Create run name in the requested format\n",
        "    wandb.run.name = \" batch_size {} augmentation {} learning_rate {}\".format(\n",
        "        cfg.batch_size,\n",
        "        cfg.augmentation,\n",
        "        cfg.learning_rate\n",
        "\n",
        "    )\n",
        "\n",
        "    # Set device (GPU if available, otherwise CPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    torch.backends.cudnn.benchmark = True  # For faster training\n",
        "\n",
        "    # Get data loaders\n",
        "    train_loader, val_loader, test_loader = get_data_loaders(cfg)\n",
        "\n",
        "    cfg.strategy = 1\n",
        "\n",
        "    # Initialize model\n",
        "    model, params = get_model(cfg.strategy)\n",
        "    model = nn.DataParallel(model)\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.NAdam(params, lr=cfg.learning_rate)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Mixed precision training for better performance\n",
        "    # Fixed to use new API format\n",
        "    scaler = GradScaler('cuda')\n",
        "\n",
        "    # Tracking metrics\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    epochs = 10  # Fixed number of epochs\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # ---------- TRAINING PHASE ----------\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        # Process batches\n",
        "        for inputs, targets in tqdm(train_loader, desc=f\"Training epoch {epoch}/{epochs}\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            # Fixed to use new API format\n",
        "            with autocast('cuda'):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass with gradient scaling\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "            predictions = outputs.argmax(1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_acc_history.append(train_accuracy)\n",
        "\n",
        "        # ---------- VALIDATION PHASE ----------\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "        # No gradient calculation needed for validation\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Calculate loss\n",
        "                val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                predictions = outputs.argmax(1)\n",
        "                val_correct += (predictions == targets).sum().item()\n",
        "                val_total += targets.size(0)\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_acc_history.append(val_accuracy)\n",
        "\n",
        "        # Log metrics to wandb\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_accuracy\": val_accuracy,\n",
        "            \"val_loss\": val_loss\n",
        "        })\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch}/{epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    # ---------- TESTING PHASE ----------\n",
        "    # Load best model for final evaluation\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "    test_correct, test_total = 0, 0\n",
        "\n",
        "    # Evaluate on test set\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            predictions = model(inputs).argmax(1)\n",
        "            test_correct += (predictions == targets).sum().item()\n",
        "            test_total += targets.size(0)\n",
        "\n",
        "    test_accuracy = 100 * test_correct / test_total\n",
        "    wandb.log({\"test_accuracy\": test_accuracy})\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "oshqwXHge7Di"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}