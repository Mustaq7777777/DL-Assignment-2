{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjsH+Kd6TwbjmKkjMfR8Dg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mustaq7777777/DL-Assignment-2/blob/main/DA6401_Assignment2_partA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required Libraries"
      ],
      "metadata": {
        "id": "F61NMYIr4IEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch.amp import GradScaler, autocast"
      ],
      "metadata": {
        "id": "vOyINfQf4HqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Configuration"
      ],
      "metadata": {
        "id": "0I1AXFAd4Y6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Mount Google Drive for data access\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "#using kaggle\n",
        "\n",
        "wandb.login(key=\"c4db2008beb715972687303f6cbced62af338b92\")\n",
        "\n",
        "# Define path to dataset\n",
        "BASE_PATH = '/content/drive/MyDrive/DL-Assignment2-data/inaturalist_12K'"
      ],
      "metadata": {
        "id": "ahtqNs0h4hGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility Functions"
      ],
      "metadata": {
        "id": "v3SHKQ6t4rrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate output dimensions after convolution operation\n",
        "def calculate_output_dimensions(input_size, kernel_size, stride=1, padding=0):\n",
        "    \"\"\"Calculate the output dimensions after applying convolution\"\"\"\n",
        "    return math.floor((input_size - kernel_size + 2*padding) / stride) + 1"
      ],
      "metadata": {
        "id": "bRlYbsEC4uA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation(train, validation and test)"
      ],
      "metadata": {
        "id": "EvAhFmPo43D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_data_loaders(cfg):\n",
        "    \"\"\"\n",
        "    Prepare data loaders for training, validation and testing\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration object containing data parameters\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_loader, val_loader, test_loader)\n",
        "    \"\"\"\n",
        "    # Define transformations based on augmentation flag\n",
        "    if cfg.augmentation:\n",
        "        # More aggressive transformations for training\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(30),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        # Basic transformations without augmentation\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # Validation transforms (no augmentation needed)\n",
        "    val_transforms = transforms.Compose([\n",
        "        transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = datasets.ImageFolder(os.path.join(BASE_PATH, 'train'), transform=train_transforms)\n",
        "    test_dataset = datasets.ImageFolder(os.path.join(BASE_PATH, 'val'), transform=val_transforms)\n",
        "\n",
        "    # Split training data to create validation set\n",
        "    indices = list(range(len(train_dataset)))\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.2,  # 20% for validation\n",
        "        stratify=train_dataset.targets,  # Maintain class distribution\n",
        "        random_state=42  # For reproducibility\n",
        "    )\n",
        "\n",
        "    # Create subsets\n",
        "    train_subset = Subset(train_dataset, train_indices)\n",
        "    val_subset = Subset(train_dataset, val_indices)\n",
        "\n",
        "    # Get number of CPU cores for worker calculation\n",
        "    num_workers = min(2, os.cpu_count() or 1)  # Use at most 2 workers to avoid warning\n",
        "\n",
        "    # Create and return data loaders\n",
        "    return (\n",
        "        DataLoader(train_subset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                   num_workers=num_workers, pin_memory=True),\n",
        "        DataLoader(val_subset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                   num_workers=num_workers, pin_memory=True),\n",
        "        DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                   num_workers=num_workers, pin_memory=True)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "seD17lJT4-Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution Nueral Network implemented as class"
      ],
      "metadata": {
        "id": "oVTkiTMh5Oyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Neural Network with configurable architecture\n",
        "    - Variable number of convolutional layers\n",
        "    - Configurable filter sizes and counts\n",
        "    - Choice of activation functions\n",
        "    - Optional batch normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Initialize lists to hold network components\n",
        "        self.conv_blocks = nn.ModuleList()\n",
        "\n",
        "        # Track dimensions for proper sizing of fully connected layer\n",
        "        in_channels = 3  # RGB input\n",
        "        current_size = cfg.img_size\n",
        "\n",
        "        # Create convolutional blocks\n",
        "        for i, (out_channels, kernel_size) in enumerate(zip(cfg.num_filters, cfg.filter_sizes)):\n",
        "            # Create a block with conv, optional batchnorm, activation, and pooling\n",
        "            block = self._create_conv_block(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                use_batchnorm=cfg.batch_norm,\n",
        "                activation=cfg.activation\n",
        "            )\n",
        "            self.conv_blocks.append(block)\n",
        "\n",
        "            # Update dimensions for next layer\n",
        "            current_size = calculate_output_dimensions(current_size, kernel_size, padding=1)\n",
        "            current_size = calculate_output_dimensions(current_size, 2, stride=2)  # pooling\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Adaptive pooling ensures fixed size regardless of input dimensions\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "        # Fully connected classification layers\n",
        "        self.classifier = self._create_classifier(\n",
        "            in_channels * 6 * 6,  # Flattened feature maps\n",
        "            cfg.fc_hidden_sizes,\n",
        "            10,  # Number of classes\n",
        "            cfg.dropout,\n",
        "            cfg.batch_norm,\n",
        "            cfg.activation\n",
        "        )\n",
        "\n",
        "    def _create_conv_block(self, in_channels, out_channels, kernel_size,\n",
        "                           use_batchnorm=True, activation='relu'):\n",
        "        \"\"\"Create a convolutional block with optional batch normalization\"\"\"\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
        "        ]\n",
        "\n",
        "        if use_batchnorm:\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        layers.append(self._get_activation_function(activation))\n",
        "        layers.append(nn.MaxPool2d(2, 2))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _create_classifier(self, in_features, hidden_size, num_classes,\n",
        "                           dropout_rate, use_batchnorm, activation):\n",
        "        \"\"\"Create the classifier part of the network\"\"\"\n",
        "        layers = [\n",
        "            nn.Linear(in_features, hidden_size),\n",
        "        ]\n",
        "\n",
        "        if use_batchnorm:\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "\n",
        "        layers.extend([\n",
        "            self._get_activation_function(activation),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        ])\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _get_activation_function(self, name):\n",
        "        \"\"\"Return the appropriate activation function based on name\"\"\"\n",
        "        activation_functions = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'gelu': nn.GELU(),\n",
        "            'silu': nn.SiLU(),\n",
        "            'mish': nn.Mish(),\n",
        "            'elu': nn.ELU(),\n",
        "            'selu': nn.SELU()\n",
        "        }\n",
        "        return activation_functions.get(name.lower(), nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        # Pass input through convolutional blocks\n",
        "        for block in self.conv_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Global pooling and flatten\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Classification\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "cOHE-rhM5Vrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "WU0t2AGjvKng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train():\n",
        "    \"\"\"Main training function that handles the entire training process\"\"\"\n",
        "    # Initialize wandb with default configuration\n",
        "    wandb.init(config=DEFAULT_CONFIG, reinit=True)\n",
        "    cfg = wandb.config\n",
        "\n",
        "    # Set fixed image size\n",
        "    cfg.img_size = 400\n",
        "\n",
        "    # Create run name in the requested format\n",
        "    wandb.run.name = \"optimizer {} activation {} num_filters {} dropout {} filter_sizes {} batch_size {} augmentation {} weight_decay {} batch_norm {}\".format(\n",
        "        cfg.optimizer,\n",
        "        cfg.activation,\n",
        "        cfg.num_filters,\n",
        "        cfg.dropout,\n",
        "        cfg.filter_sizes,\n",
        "        cfg.batch_size,\n",
        "        cfg.augmentation,\n",
        "        cfg.weight_decay,\n",
        "        cfg.batch_norm\n",
        "    )\n",
        "\n",
        "    # Set device (GPU if available, otherwise CPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    torch.backends.cudnn.benchmark = True  # For faster training\n",
        "\n",
        "    # Get data loaders\n",
        "    train_loader, val_loader, test_loader = get_data_loaders(cfg)\n",
        "\n",
        "    # Initialize model\n",
        "    model = CNN(cfg)\n",
        "    model = nn.DataParallel(model)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Select optimizer based on configuration\n",
        "    if cfg.optimizer.lower() == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "    elif cfg.optimizer.lower() == 'nadam':\n",
        "        optimizer = optim.NAdam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "    elif cfg.optimizer.lower() == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "    else:\n",
        "        # Default to NAdam if unspecified\n",
        "        optimizer = optim.NAdam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Mixed precision training for better performance\n",
        "    # Fixed to use new API format\n",
        "    scaler = GradScaler('cuda')\n",
        "\n",
        "    # Tracking metrics\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    epochs = 10  # Fixed number of epochs\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # ---------- TRAINING PHASE ----------\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        # Process batches\n",
        "        for inputs, targets in tqdm(train_loader, desc=f\"Training epoch {epoch}/{epochs}\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            # Fixed to use new API format\n",
        "            with autocast('cuda'):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass with gradient scaling\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "            predictions = outputs.argmax(1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_acc_history.append(train_accuracy)\n",
        "\n",
        "        # ---------- VALIDATION PHASE ----------\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "        # No gradient calculation needed for validation\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Calculate loss\n",
        "                val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                predictions = outputs.argmax(1)\n",
        "                val_correct += (predictions == targets).sum().item()\n",
        "                val_total += targets.size(0)\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_acc_history.append(val_accuracy)\n",
        "\n",
        "        # Log metrics to wandb\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_accuracy\": val_accuracy,\n",
        "            \"val_loss\": val_loss\n",
        "        })\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch}/{epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    # ---------- TESTING PHASE ----------\n",
        "    # Load best model for final evaluation\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "    test_correct, test_total = 0, 0\n",
        "\n",
        "    # Evaluate on test set\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            predictions = model(inputs).argmax(1)\n",
        "            test_correct += (predictions == targets).sum().item()\n",
        "            test_total += targets.size(0)\n",
        "\n",
        "    test_accuracy = 100 * test_correct / test_total\n",
        "    wandb.log({\"test_accuracy\": test_accuracy})\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "wIj3_9C6vNGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sweep"
      ],
      "metadata": {
        "id": "fk2g0KwhvX8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Hyperparameter sweep configuration\n",
        "    sweep_configuration = {\n",
        "        'method': 'bayes',  # Bayesian optimization for efficient hyperparameter search\n",
        "        'name': 'cnn-hyperparameter-tuning',\n",
        "        'metric': {\n",
        "            'name': 'val_accuracy',  # Metric to optimize\n",
        "            'goal': 'maximize'  # We want to maximize accuracy\n",
        "        },\n",
        "        'parameters': {\n",
        "            # Different filter configurations\n",
        "            'num_filters': {\n",
        "                'values': [\n",
        "                    [64, 128, 256, 512, 1024],  # Wide architecture\n",
        "                    [32, 32, 32, 32, 32],       # Uniform width\n",
        "                    [32, 64, 64, 128, 128],     # Gradually increasing\n",
        "                    [128, 128, 64, 64, 32],     # Gradually decreasing\n",
        "                    [32, 64, 128, 256, 512]     # Standard pyramid\n",
        "                ]\n",
        "            },\n",
        "            # Kernel size variations\n",
        "            'filter_sizes': {\n",
        "                'values': [\n",
        "                    [3, 3, 3, 3, 3],  # All small kernels\n",
        "                    [5, 5, 5, 5, 5],  # All large kernels\n",
        "                    [5, 3, 5, 3, 5]   # Mixed kernel sizes\n",
        "                ]\n",
        "            },\n",
        "            # Regularization strength\n",
        "            'weight_decay': {\n",
        "                'values': [0, 0.0005, 0.5]\n",
        "            },\n",
        "            # Data augmentation toggle\n",
        "            'augmentation': {\n",
        "                'values': [True, False]\n",
        "            },\n",
        "            # Dropout rates\n",
        "            'dropout': {\n",
        "                'values': [0, 0.2, 0.4]\n",
        "            },\n",
        "            # Learning rates\n",
        "            'learning_rate': {\n",
        "                'values': [1e-3, 1e-4]\n",
        "            },\n",
        "            # Activation functions\n",
        "            'activation': {\n",
        "                'values': ['relu', 'elu', 'selu', 'silu', 'gelu', 'mish']\n",
        "            },\n",
        "            # Optimizer choices\n",
        "            'optimizer': {\n",
        "                'values': ['nadam', 'adam', 'rmsprop']\n",
        "            },\n",
        "            # Batch normalization toggle\n",
        "            'batch_norm': {\n",
        "                'values': [True, False]\n",
        "            },\n",
        "            # Batch sizes\n",
        "            'batch_size': {\n",
        "                'values': [32, 64]\n",
        "            },\n",
        "            # Fully connected layer sizes\n",
        "            'fc_hidden_sizes': {\n",
        "                'values': [128, 256, 512]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Initialize and run sweep\n",
        "    sweep_id = wandb.sweep(sweep_configuration, project=\"DA6401-Assignment-2\")\n",
        "    wandb.agent(\"jvw6z1oy\", function=train, count=20)  # Run 20 trials"
      ],
      "metadata": {
        "id": "GxixQXq6val0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}